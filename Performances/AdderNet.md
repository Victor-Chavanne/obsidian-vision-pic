Entrainement de modèles  de deep-learning et CNN à l'aide d'additions.Alternatives au calcul matriciel
White Papers : 
- 2020 [AdderNet: Do We Really Need Multiplications in Deep Learning?](https://arxiv.org/pdf/1912.13200v5.pdf)
- 2021 [AdderNet and its Minimalist Hardware Design for Energy-Efficient Artificial Intelligence](https://arxiv.org/pdf/2101.10015v2.pdf)

Plus de 99% d'accuracy sur le MNIST dataset pour une fraction des ressources consomées.
1.16×  speed-up ratio, 67.6%-71.4% decrease in logic resource utilization (chip area), 47.85%-77.9% decrease in power consumption compared to CNN

[GitHub](https://github.com/huawei-noah/AdderNet)
[Yunhe Wang](https://www.wangyunhe.site)

To read :
[iMAD: An In-Memory Accelerator for AdderNet with Efficient 8-bit Addition and Subtraction Operations](https://dl.acm.org/doi/abs/10.1145/3526241.3530313)
[Converting Novel Neural Network Architectures to TensorRT — AdderNet to TensorRT](https://medium.com/analytics-vidhya/converting-novel-neural-network-architectures-to-tensorrt-addernet-to-tensorrt-20b11d7fe5cf)
[Review — AdderNet: Do We Really Need Multiplications in Deep Learning? (Image Classification)](https://sh-tsang.medium.com/review-addernet-do-we-really-need-multiplications-in-deep-learning-image-classification-b72851ddb255)
